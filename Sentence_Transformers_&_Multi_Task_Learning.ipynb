{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 1:** Sentence Transformer Implementation\n",
        "Implementation Choices\n",
        "\n",
        "For the sentence transformer, I'll use **PyTorch** with the **HuggingFace** **transformers** library, which provides pre-trained transformer models and makes implementation straightforward.\n",
        "\n",
        "Key architectural choices:\n",
        "\n",
        "* Model Selection: Using distilbert-base-uncased as the backbone - it's lighter than BERT but maintains good performance\n",
        "\n",
        "* Pooling Method: Mean pooling of the last hidden states to get fixed-length embeddings\n",
        "\n",
        "* Normalization: L2 normalization of output embeddings for consistent scaling"
      ],
      "metadata": {
        "id": "9AxZoyrADB0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from torch import nn\n",
        "from typing import List\n",
        "\n",
        "class SentenceTransformer(nn.Module):\n",
        "    def __init__(self, model_name: str = \"distilbert-base-uncased\"):\n",
        "        super().__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "    def forward(self, sentences: List[str]):\n",
        "        # Tokenize input sentences\n",
        "        inputs = self.tokenizer(\n",
        "            sentences,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Move inputs to the same device as model\n",
        "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
        "\n",
        "        # Get model outputs\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "\n",
        "        # Mean pooling\n",
        "        last_hidden_state = outputs.last_hidden_state\n",
        "        attention_mask = inputs[\"attention_mask\"].unsqueeze(-1)\n",
        "        mean_pooled = (last_hidden_state * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
        "\n",
        "        # L2 normalization\n",
        "        normalized_embeddings = torch.nn.functional.normalize(mean_pooled, p=2, dim=1)\n",
        "\n",
        "        return normalized_embeddings\n",
        "\n",
        "    def encode(self, sentences: List[str]):\n",
        "        self.eval()\n",
        "        return self.forward(sentences)"
      ],
      "metadata": {
        "id": "yQkEpItKIUph"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing the Implementation"
      ],
      "metadata": {
        "id": "iSdGM34TIaLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SentenceTransformer().to(device)\n",
        "\n",
        "# Sample sentences\n",
        "sentences = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Artificial intelligence is transforming industries.\",\n",
        "    \"Sentence transformers are useful for NLP tasks.\"\n",
        "]\n",
        "\n",
        "# Get embeddings\n",
        "embeddings = model.encode(sentences)\n",
        "print(f\"Embeddings shape: {embeddings.shape}\")  # Should be (3, 768) for 3 sentences\n",
        "print(f\"Sample embedding (first 5 dims): {embeddings[0][:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y70L4UndIWcc",
        "outputId": "ddf37924-23bd-43be-ea73-088965cdd50f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings shape: torch.Size([3, 768])\n",
            "Sample embedding (first 5 dims): tensor([-0.0119, -0.0021, -0.0034,  0.0203,  0.0344], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This output confirms that the sentence transformer is working correctly.\n",
        "* Each of the 3 input sentences has been converted into a 768-dimensional embedding, consistent with the BERT-base model's output size.\n",
        "* These fixed-size vector representations can now be used as inputs for the multi-task model in Task 2."
      ],
      "metadata": {
        "id": "BbCtn6jLjNmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Justification of Choices\n",
        "* **DistilBERT**: Chosen for its balance between performance and efficiency. It's 40% smaller than BERT but retains 97% of its performance.\n",
        "\n",
        "* **Mean Pooling**: Simple yet effective way to aggregate token embeddings into sentence embeddings. Alternative would be max pooling or using [CLS] token.\n",
        "\n",
        "* **L2 Normalization**: Makes embeddings more suitable for similarity comparisons (cosine similarity becomes dot product)."
      ],
      "metadata": {
        "id": "5F64pHpFLmzs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 2**: Multi-Task Learning Expansion\n",
        "Architecture Expansion\n",
        "I'll expand the model to handle:\n",
        "\n",
        "* **Task A**: Sentence Classification (3 classes: \"Technology\", \"Science\", \"Other\")\n",
        "\n",
        "* **Task B**: Sentiment Analysis (3 classes: \"Positive\", \"Negative\", \"Neutral\")"
      ],
      "metadata": {
        "id": "o2zn6NbVJADp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiTaskSentenceTransformer(nn.Module):\n",
        "    def __init__(self, model_name: str = \"distilbert-base-uncased\"):\n",
        "        super().__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.backbone = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "        # Task A: Sentence Classification head\n",
        "        self.classification_head = nn.Sequential(\n",
        "            nn.Linear(self.backbone.config.hidden_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, 3)  # 3 classes for Task A\n",
        "        )\n",
        "\n",
        "        # Task B: Sentiment Analysis head\n",
        "        self.sentiment_head = nn.Sequential(\n",
        "            nn.Linear(self.backbone.config.hidden_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, 3)  # 3 classes for Task B\n",
        "        )\n",
        "\n",
        "    def forward(self, sentences: List[str]):\n",
        "        # Tokenize input sentences\n",
        "        inputs = self.tokenizer(\n",
        "            sentences,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        inputs = {k: v.to(self.backbone.device) for k, v in inputs.items()}\n",
        "\n",
        "        # Get backbone outputs\n",
        "        backbone_outputs = self.backbone(**inputs)\n",
        "        last_hidden_state = backbone_outputs.last_hidden_state\n",
        "\n",
        "        # Mean pooling for sentence representation\n",
        "        attention_mask = inputs[\"attention_mask\"].unsqueeze(-1)\n",
        "        mean_pooled = (last_hidden_state * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
        "\n",
        "        # Task outputs\n",
        "        classification_logits = self.classification_head(mean_pooled)\n",
        "        sentiment_logits = self.sentiment_head(mean_pooled)\n",
        "\n",
        "        return {\n",
        "            \"sentence_embedding\": mean_pooled,\n",
        "            \"classification_logits\": classification_logits,\n",
        "            \"sentiment_logits\": sentiment_logits\n",
        "        }"
      ],
      "metadata": {
        "id": "75qnjRmTdBMr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Justification of Changes\n",
        "1. **Shared Backbone**: Both tasks share the same transformer backbone for feature extraction, enabling knowledge transfer.\n",
        "\n",
        "2. **Task-Specific Heads**: Separate heads allow each task to learn specialized representations while sharing common features.\n",
        "\n",
        "3. **Hidden Layers**: Added small feedforward networks (256 units) before final classification to allow for task-specific feature transformation.\n",
        "\n",
        "4. **Dropout**: Added for regularization to prevent overfitting in task heads."
      ],
      "metadata": {
        "id": "UROqjvT9MRRh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 3 - Training Strategy Analysis**\n",
        "#### **A) Implications & Advantages of Different Training Scenarios**\n",
        "1.**Frozen Entire Network Approach**\n",
        "\n",
        "**Definition**: All model parameters (transformer backbone and task heads) remain fixed during training.\n",
        "\n",
        "**Key Implications**:\n",
        "\n",
        "* Zero updates to any layer weights\n",
        "\n",
        "* Only the final classification layers make predictions based on frozen features\n",
        "\n",
        "* Model acts as a fixed feature extractor\n",
        "\n",
        "**Advantages**:\n",
        "\n",
        "* **Exceptional Training Speed**: No backpropagation through heavy transformer layers\n",
        "\n",
        "* **Maximum Stability**: Preserves all pre-trained knowledge perfectly\n",
        "\n",
        "* **Strong Baseline**: Leverages BERT's robust linguistic understanding without modification\n",
        "\n",
        "* **Overfitting Protection**: Ideal for extremely small datasets (less than 100 examples per class)\n",
        "\n",
        "* **Best Use Case**: When working with minimal labeled data and the pre-trained model's representations are already well-aligned with your tasks.\n",
        "\n",
        "2.**Frozen Backbone with Trainable Heads**\n",
        "\n",
        "**Definition**: Transformer layers remain fixed while task-specific classification heads learn.\n",
        "\n",
        "**Key Implications**:\n",
        "\n",
        "* Backbone outputs consistent features\n",
        "\n",
        "* Heads adapt to interpret these features for specific tasks\n",
        "\n",
        "* Moderate parameter updates during training\n",
        "\n",
        "**Advantages**:\n",
        "\n",
        "* **Balanced Approach**: Combines stable features with task adaptation\n",
        "\n",
        "* **Efficient Training**: 90%+ fewer trainable parameters than full fine-tuning\n",
        "\n",
        "* **Controlled Specialization**: Prevents catastrophic forgetting of general language knowledge\n",
        "\n",
        "* **Proven Effectiveness**: Standard approach for most NLP transfer learning cases\n",
        "\n",
        "* **Best Use Case**: The default recommendation for datasets with 100-10,000 examples where some task-specific adaptation is beneficial.\n",
        "\n",
        "3.**Partially Frozen Architecture** (Single Head Frozen)\n",
        "\n",
        "**Definition**: One task head remains fixed while the backbone and other head train.\n",
        "\n",
        "**Key Implications**:\n",
        "\n",
        "* Asymmetric learning across tasks\n",
        "\n",
        "* Dynamic interplay between fixed and adapting components\n",
        "\n",
        "* Complex gradient flow patterns\n",
        "\n",
        "**Advantages**:\n",
        "\n",
        "* **Targeted Adaptation**: Focuses learning on the most important/underperforming task\n",
        "\n",
        "* **Interference Control**: Prevents one task from disrupting another's performance\n",
        "\n",
        "* **Incremental Learning**: Allows phased introduction of new tasks\n",
        "\n",
        "* **Resource Allocation**: Efficient when tasks have unequal data quantities\n",
        "\n",
        "**Best Use Case**: When tasks have imbalanced requirements (e.g., one well-performing task with ample data and one new task with limited data).\n"
      ],
      "metadata": {
        "id": "SLstQxC_VDYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **B) Transfer Learning Approach in NLP Scenarios**\n",
        "Transfer learning is especially valuable when working with limited labeled data or when the target task resembles tasks learned by large pre-trained models. Here's how I would approach it:\n",
        "\n",
        "1. **Choosing a Pre-trained Model**\n",
        "* The choice of pre-trained model depends heavily on the domain, the type of data, and the scale of the dataset. In this project, we are dealing with a small, custom-built dataset and a task that falls within the realm of natural language processing (NLP). For such scenarios, BERT-base is an excellent choice because it has been trained on a massive corpus of general English text and is known to effectively capture a wide range of linguistic patterns, semantics, and syntactic structures. This makes it a powerful foundation for downstream tasks like sentence classification or sentiment analysis.\n",
        "\n",
        "2. **Freezing and Unfreezing Layers**\n",
        "When applying transfer learning, it's important to balance the general knowledge embedded in the pre-trained model with the need for task-specific adaptation. My strategy would be:\n",
        "\n",
        "* Freeze the lower layers of BERT (e.g., embedding and early transformer blocks). These layers capture fundamental language features like syntax and common phrase patterns, which are generally applicable across most NLP tasks. Freezing them reduces the risk of overfitting, especially when working with small datasets.\n",
        "\n",
        "* Fine-tune the upper layers and task-specific heads (e.g., classification and sentiment analysis layers). These higher layers are more specialized and can be adapted to the nuances of the specific downstream task. Fine-tuning them enables the model to learn task-specific representations while preserving the general linguistic understanding from pre-training."
      ],
      "metadata": {
        "id": "9OLhxAHQfFNs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tRP0BwSAjmvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 4: Training Loop Implementation (BONUS)**"
      ],
      "metadata": {
        "id": "7EGtX_Nnmf8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data: [sentence, classification_label (Task A), sentiment_label (Task B)]\n",
        "sample_data = [\n",
        "    (\"AI is transforming industries.\", 0, 0),       # Technology, Positive\n",
        "    (\"The universe is expanding rapidly.\", 1, 1),    # Science, Negative\n",
        "    (\"The weather is quite nice today.\", 2, 0),      # Other, Positive\n",
        "    (\"Quantum physics is mind-bending.\", 1, 2),      # Science, Neutral\n",
        "    (\"New phones have amazing features.\", 0, 0),     # Technology, Positive\n",
        "    (\"I dislike the slow internet speed.\", 2, 1),    # Other, Negative\n",
        "]\n"
      ],
      "metadata": {
        "id": "7nGYntEbJg8H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SentenceDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data  # List of tuples: (sentence, class_label, sentiment_label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence, class_label, sentiment_label = self.data[idx]\n",
        "        return {\n",
        "            \"sentence\": sentence,\n",
        "            \"classification_label\": torch.tensor(class_label, dtype=torch.long),\n",
        "            \"sentiment_label\": torch.tensor(sentiment_label, dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "eR0_v2nSmqn-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Use the earlier defined model\n",
        "model = MultiTaskSentenceTransformer().to(device)\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = SentenceDataset(sample_data)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Training Loop\n",
        "epochs = 4\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    correct_class, correct_sentiment, total = 0, 0, 0\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=f\"Epoch {epoch + 1}\"):\n",
        "        sentences = batch[\"sentence\"]\n",
        "        class_labels = batch[\"classification_label\"].to(device)\n",
        "        sentiment_labels = batch[\"sentiment_label\"].to(device)\n",
        "\n",
        "        outputs = model(sentences)\n",
        "        class_logits = outputs[\"classification_logits\"]\n",
        "        sentiment_logits = outputs[\"sentiment_logits\"]\n",
        "\n",
        "        # Losses\n",
        "        loss_class = F.cross_entropy(class_logits, class_labels)\n",
        "        loss_sentiment = F.cross_entropy(sentiment_logits, sentiment_labels)\n",
        "        loss = loss_class + loss_sentiment  # Equal weighting\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Accuracy\n",
        "        class_preds = torch.argmax(class_logits, dim=1)\n",
        "        sentiment_preds = torch.argmax(sentiment_logits, dim=1)\n",
        "        correct_class += (class_preds == class_labels).sum().item()\n",
        "        correct_sentiment += (sentiment_preds == sentiment_labels).sum().item()\n",
        "        total += class_labels.size(0)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Loss={total_loss:.4f}, \"\n",
        "          f\"Classification Accuracy={correct_class/total:.2f}, Sentiment Accuracy={correct_sentiment/total:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iXF42S5m0WT",
        "outputId": "267ec2d0-d401-4221-b232-1c533012d89d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 3/3 [00:00<00:00, 24.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss=6.5918, Classification Accuracy=0.50, Sentiment Accuracy=0.50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 3/3 [00:00<00:00, 26.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Loss=6.4482, Classification Accuracy=0.33, Sentiment Accuracy=0.67\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 3/3 [00:00<00:00, 27.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Loss=6.0521, Classification Accuracy=0.83, Sentiment Accuracy=0.83\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 3/3 [00:00<00:00, 26.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Loss=5.8148, Classification Accuracy=0.83, Sentiment Accuracy=0.83\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Observations**\n",
        "* **Loss Decrease**: The training loss consistently decreased from 6.59 to 5.81 over 4 epochs, indicating the model is learning effectively.\n",
        "\n",
        "* **Accuracy Trends**:\n",
        "\n",
        " 1. Classification Accuracy improved from 50% to 83% by epoch 3 and stabilized, showing the model is learning task A well.\n",
        " 2. Sentiment Accuracy followed a similar trend, also reaching 83%, indicating balanced learning across both tasks.\n",
        "\n",
        "* **Multi-Task Learning Working**: Shared transformer backbone with task-specific heads is performing well, with neither task dominating.\n",
        "\n",
        "* **Small Dataset**: The model was trained on a very limited dataset (6 examples), making the high accuracy likely due to overfitting or memorization.\n",
        "\n",
        "* **Training Stability**: Metrics stabilized after epoch 3, suggesting good convergence behavior even with minimal data."
      ],
      "metadata": {
        "id": "9zoOBjTfooVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final Thoughts**\n",
        "* The implementation correctly showcases a working multi-task learning (MTL) architecture with a shared encoder and task-specific heads.\n",
        "\n",
        "* The model demonstrates the ability to improve both classification and sentiment predictions simultaneously."
      ],
      "metadata": {
        "id": "XfC16-xIpST5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V9ClvdoPnAm1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}